{"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"markdown","source":["# Capstone Project\nExported from Filament on Tue, 12 Apr 2022 15:25:13 GMT\n\n---"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd \nimport numpy as np \nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\nimport statsmodels.api as sm\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n# from sklearn import svm\n# from sklearn.svm import SVC\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\n\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport os"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# reading in f1_csv files\nf1_csv = [\n            'drivers.csv', 'results.csv', 'driver_standings.csv',\n            'constructors.csv', 'constructor_results.csv',\n            'constructor_standings.csv', 'races.csv'\n          ]\n\ndrivers_cols = ['driverId', 'forename', 'surname', 'dob',\n                'nationality']\nresults_cols = ['resultId', 'raceId', 'driverId', 'constructorId',\n                'grid', 'positionOrder', 'points']\ndriver_standings_cols = ['driverStandingsId', 'raceId', 'driverId',\n                        'points', 'position', 'wins']\nconstructors_cols = ['constructorId', 'name', 'nationality']\nconstructor_results_cols = ['constructorResultsId', 'raceId',\n                           'constructorId', 'points']\nconstructor_standings_cols = ['constructorStandingsId', 'raceId',\n                              'constructorId', 'points', 'position',\n                              'positionText', 'wins']\nraces_cols = ['raceId', 'year', 'round']\n\ndrivers = pd.read_csv('drivers.csv', usecols=drivers_cols)\ndrivers = drivers.set_index('driverId')\n\nresults = pd.read_csv('results.csv', usecols=results_cols)\n\ndriver_standings = pd.read_csv('driver_standings.csv', usecols=driver_standings_cols)\n\nconstructors = pd.read_csv('constructors.csv', usecols=constructors_cols)\nconstructors = constructors.set_index('constructorId')\n\nconstructor_results = pd.read_csv('constructor_results.csv', \n                                  usecols=constructor_results_cols)\n\nconstructor_standings = pd.read_csv('constructor_standings.csv',\n                                   usecols=constructor_standings_cols)\n\nraces = pd.read_csv('races.csv', usecols=races_cols)\nraces = races.set_index('raceId')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# checking for nulls and datatype in each dataframe\nf1_dfs = [drivers, results, driver_standings, constructors,\n          constructor_results, constructor_standings, races]\n\nfor i in f1_dfs:\n    print(i.dtypes)\n    for columns in i.columns:  \n        nulls = i[columns].isna().sum()\n        print(f'{columns}: {nulls}')\n    print('\\n')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["## This was checking all erronous results that will be used in\n## this Series are assigned an E for expulsion\nconstructor_standings[constructor_standings.positionText == 'E']"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["## The following code was to assertain strange driver\n## rankings from odd years of f1, this is also a reason we look\n## at the last 20 years, although 1996 onwards would be acceptable\n## as well\n\ndriver_standings.position.unique()"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":[""],"metadata":{}},{"cell_type":"markdown","source":["## Working on 1 year of data for proof of concept"],"metadata":{}},{"cell_type":"code","source":["results_race = results.join(races, on='raceId')\nresults_2002 = results_race.loc[results_race.year == 2002]\nresults_2002['podium'] = results_2002.positionOrder.map(lambda x: 1 if x<=3 else 0)\nresults_2002['win'] = results_2002.positionOrder.map(lambda x: 1 if x==1 else 0)\n\nresults_2002 = results_2002.groupby(['year','driverId']).agg({\n    'points': 'sum',\n    'positionOrder': 'median',\n    'podium': 'sum', \n    'win': 'sum',\n    'grid': 'median', \n    'driverId': 'count',\n    'constructorId': lambda x:x.value_counts().index[0]\n                }).rename(columns={\n                            'positionOrder': 'median_position',\n                            'grid': 'median_start_position',\n                            'podium': 'podiums',\n                            'win': 'wins',\n                            'driverId': 'num_races'\n                                        })\nresults_2002['percentage_races'] = results_2002['num_races'] / results_2002['num_races'].max()\nresults_2002['full_season'] = results_2002.percentage_races.map(lambda x: 1 if x==1 else 0)\n\n\n## Working on driver rankings for one season\nds = driver_standings.join(races, on='raceId')\nds2002 = ds.loc[ds.year == 2002]\nds2002 = ds2002.loc[ds2002['round'] == ds2002['round'].max()]\nds2002 = ds2002[['driverId', 'position']].set_index('driverId')\n\n## Working on team rankings for one season\ncs = constructor_standings.join(races, on='raceId')\ncs2002 = cs.loc[cs.year == 2002]\ncs2002 = cs2002.loc[cs2002['round'] == cs2002['round'].max()]\ncs2002 = cs2002[['constructorId', 'points', 'position']].set_index('constructorId')\n\n## join our information\nresults_2002 = results_2002.join(ds2002, on='driverId')\nresults_2002 = results_2002.join(cs2002, on='constructorId',\n                                 lsuffix='_driver', rsuffix='_constructor')\nresults_2002['position_driver'] = results_2002['position_driver'].fillna((results_2002['position_driver'].max() + 1))\nresults_2002.reset_index()"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## Function to enable DF generation for any year"],"metadata":{}},{"cell_type":"code","source":["def driver_attributes_year(input_year):\n    '''\n    Generates a data frame for analysis with the following Series\n    attributes\n    \n    year: season of championship\n    driverId: driverId number, information in the drivers.csv file\n    points_driver: sum of driver points for that season\n    median_position: median final position of driver for the season\n    podiums: number of season podiums (position = 1, 2, 3)\n    wins: number of season wins (position = 1)\n    median_start_position: median qualifying position of the driver\n        (this is post sprint race result for 2021 onwards)\n    num_races: number of race weekends entered\n        (some drivers do not qualify fast enough for the race)\n    percentage_races: percent of races taken part in\n    full_season: driver complete a full season (1 = yes, 0 = no)\n    constructorId: or team, Id number, information in the\n        constructors.csv file. This is the team the driver raced for\n        most in the season (modal class)\n    position_driver: season ending rank of the driver\n    points_constructor: number of constructor points that season \n    position_constructor: season ending rank of the constructor\n\n    input_year - format: int, the championship year dataframe \n                            to be generated\n    '''\n    # removing slice warning\n    pd.options.mode.chained_assignment = None\n\n    # working on driver result information for the season\n    res_race = results.join(races, on='raceId')\n    res_year = res_race.loc[res_race.year == input_year]\n    res_year['podium'] = res_year.positionOrder.map(lambda x: 1 if x<=3 else 0)\n    res_year['win'] = res_year.positionOrder.map(lambda x: 1 if x==1 else 0)  \n    res_year = res_year.groupby(['year','driverId']).agg(\n        {\n        'points': 'sum',\n        'positionOrder': 'median',\n        'podium': 'sum', \n        'win': 'sum',\n        'grid': 'median', \n        'driverId': 'count',\n        'constructorId': lambda x:x.value_counts().index[0]\n                    }).rename(columns={\n                                'positionOrder': 'median_position',\n                                'grid': 'median_start_position',\n                                'podium': 'podiums',\n                                'win': 'wins',\n                                'driverId': 'num_races'\n                                            })\n    res_year['percentage_races'] = res_year['num_races'] / res_year['num_races'].max()\n    res_year['full_season'] = res_year.percentage_races.map(lambda x: 1 if x==1 else 0)\n    \n    ## Working on driver rankings for one season\n    ds = driver_standings.join(races, on='raceId')\n    ds_year = ds.loc[ds.year == input_year]\n    ds_year = ds_year.loc[ds_year['round'] == ds_year['round'].max()]\n    ds_year = ds_year[['driverId',\n                       'position']].set_index('driverId')\n    \n    ## Working on team rankings for one season\n    cs = constructor_standings.join(races, on='raceId')\n    cs_year = cs.loc[cs.year == input_year]\n    cs_year = cs_year.loc[cs_year['round'] == cs_year['round'].max()]\n    cs_year = cs_year[['constructorId',\n                       'points',\n                       'position']].set_index('constructorId')\n    \n    ## join our information\n    res_year = res_year.join(ds_year, on='driverId')\n    res_year = res_year.join(cs_year,\n                             on='constructorId',\n                             lsuffix='_driver',\n                             rsuffix='_constructor')\n    res_year['position_driver'] = res_year['position_driver'].fillna((results_2002['position_driver'].max() + 1))\n    return res_year.reset_index()\n\n\ndef analysis_years(start=2002, stop=2021):\n    '''\n    A range of year values of which to generate a concatenated \n    dataframe for. Column information is found in doc_string of\n    driver_attribute_year function\n\n    start = format: int, first year of data frame\n    stop = format: int, last year of data frame\n\n    Default values are start=2002 and stop=2021, generating a\n    current 20 year period\n    \n    '''\n    df = driver_attributes_year(start)\n    for i in range(start+1, stop+1):\n        df1 = driver_attributes_year(i)\n        df = pd.concat([df, df1], ignore_index=True)\n    return df\n        "],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["twenty_year_results = analysis_years()\ntwenty_year_results.to_csv('twenty_year_results.csv')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["df = pd.read_csv('twenty_year_results.csv', index_col=0)\ndf = df.join(drivers, on='driverId')\ndf['dob'] = pd.to_datetime(df['dob'], format=\"%Y-%m-%d\")\ndf = df.join(constructors, on='constructorId', lsuffix ='_driver', rsuffix = '_constructor')\ndf['driver_move'] = 0\ndf.info()"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# Calculates the churn value, based on if a duplicate exists for\n# driver and constructor pair in the following year. \nfor i in range(df.year.min(), df.year.max() + 1):\n    driver_change = pd.Series(df[(df.year <= i+1) & (df.year >= i)].duplicated(keep='last', subset=['driverId', 'constructorId']), name='bool')\n    driver_change = driver_change.to_dict()\n    for k, v in driver_change.items():\n        if v == True:\n            df.driver_move[k] = 0\n        elif v == False:\n            df.driver_move[k] = 1\n             \n# Will always assume last year leave as it has nothing to compare\n# it to. These are the indexes of remaining drivers 2021 in df to \n# set to 0\nremain_final_year_list = [460, 461, 464, 465, 466, 468, 469, 470,\n                    471, 473, 474, 475, 477, 478, 479, 480]\ndf.loc[remain_final_year_list, 'driver_move'] = 0"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["print(f\"Stay 0 = {round(256/481, 4)*100}%\")\nprint(f\"Leave 1 = {round(225/481, 4)*100}%\")\ndf.groupby('driver_move').agg({'driver_move': 'count'})"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["Our data is generally equally distributed between 0 and 1 outcome, so our base metric will be accuracy. F1 score will also be used as metric as well"],"metadata":{}},{"cell_type":"markdown","source":["## Feature Engineering"],"metadata":{}},{"cell_type":"code","source":["df['driver_age'] = df.year - pd.DatetimeIndex(df['dob']).year\ndf['percentage_of_constructor'] = df.points_driver.div(df.points_constructor).fillna(0).map(lambda x: 1 if x>1 else x)\ndf['position_gain'] = df.median_start_position - df.median_position\ndf['nationality_match'] = np.where(df.nationality_driver == df.nationality_constructor, 1, 0)\ndf['out_perform_constructor'] = (df.position_constructor*2) - df.position_driver\ndf['podium_scored'] = np.where(df.podiums >= 1, 1, 0)\ndf.head()"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## Functions for modelling"],"metadata":{}},{"cell_type":"code","source":["def conf_matrix(matrix):\n    '''\n    A function for the genration of a confusion matrix heatmap\n    \n    inputs:\n        matrix: as type (numpy.ndarray)\n    '''\n    ax = plt.subplot()\n    sns.heatmap(matrix, annot = True, ax = ax, fmt = 'g', vmin=0, cmap='crest',\n               annot_kws={\"fontsize\":30})\n    ax.xaxis.set_ticklabels(['predicted remainer (0)', 'predicted leaver (1)'])\n    # plt.setp( ax.xaxis.get_majorticklabels(), rotation=-45, ha=\"left\" )\n\n    plt.setp(ax.yaxis.set_ticklabels(['actual remainer (0)', 'actual leaver(1)']), va='center')\n\ndef a_p_r(y_pred, y_real):\n    '''\n    Generates the following metrics and prints them.\n        Accuracy: number of correct/total predictions\n        Precision: true positive/true positive + false positive\n        Recall: true positive/true positive + false negative\n        F1 score: 2*(precision*recall / precision+recall)\n    \n    inputs:\n        y_pred: pd.Series object\n        y_real: pd.Series object\n    '''\n    accuracy = metrics.accuracy_score(y_real, y_pred)\n    precision = metrics.precision_score(y_real, y_pred)\n    recall = metrics.recall_score(y_real, y_pred)\n    f1 = metrics.f1_score(y_real, y_pred)\n    print(f\"Accuracy:{accuracy}\")\n    print(f\"Precision:{precision}\")\n    print(f\"Recall:{recall}\")\n    print(f\"F1:{f1}\")\n    return accuracy, precision, recall, f1"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["number_df_cols = feature_cols = [\n                # 'year',\n                # 'driverId',\n                'points_driver',\n                'median_position',\n                'podiums',\n                'wins', \n                'median_start_position', \n                # 'num_races',\n                'percentage_races',\n                'full_season',\n                # 'constructorId',\n                'position_driver',\n                'points_constructor',\n                'position_constructor',\n                # 'forename',\n                # 'surname',\n                # 'dob',\n                # 'nationality_driver',\n                'driver_age',\n                # 'constructorRef',\n                # 'name',\n                # 'nationality_constructor',\n                # 'driver_move',\n                'percentage_of_constructor', \n                'position_gain',\n                'nationality_match',\n                'podium_scored',\n                'out_perform_constructor'\n                ]\nnumber_df = df[number_df_cols]\nplt.figure(figsize=(10,5))\nsns.heatmap(number_df.corr(),cbar=True,fmt =' .2f', annot=True, cmap='coolwarm')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## Data Prepped for modelling"],"metadata":{}},{"cell_type":"code","source":["df.shape\nprint(df.columns)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["removed_cols = [\n    'year',\n    'driverId',\n    'constructorId',\n    'forename',\n    'surname',\n    'dob',\n    'nationality_driver',\n    'name',\n    'nationality_constructor',\n    'driver_move',\n    ]\n\nfeatured_cols = [x for x in df.columns if x not in removed_cols]\nX = df[featured_cols]\ny = df.driver_move\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                        y,\n                                        test_size = 0.25,\n                                        random_state = 22,\n                                                   )"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## Exploratory Data analysis "],"metadata":{}},{"cell_type":"code","source":["ax = sns.countplot(x=y_train, data=X_train, hue='full_season')\nax.set(xlabel= None, ylabel = \"Count of Results\")\nax.set_xticklabels(['Driver Remain', 'Driver Leave'])\nplt.legend(title='Full Season', loc='upper right', labels=['No', 'Yes'])\nplt.show()"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["ax = sns.countplot(x=y_train, data=X_train, hue='nationality_match')\nax.set(xlabel= None, ylabel = \"Count of Results\")\nax.set_xticklabels(['Driver Remain', 'Driver Leave'])\nplt.legend(title='Nationality Match', loc='upper right', labels=['No', 'Yes'])\nplt.show()"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["x_input = 'median_position'\nsns.histplot(x = x_input, hue = y_train,\n                  data = X_train, multiple='dodge').set(xlabel='Median Position',\n                                                ylabel = \"Count of Results\")\nplt.legend(title='Driver Leaves?', loc='upper right', labels=['Yes', 'No'])\nplt.figure(figsize=(30,10))\nplt.show()\n# plt.figure(figsize=(35,5))\n# ((X_train[y_train==1].position_gain.value_counts().sort_index()/len(y_train))*100).plot(kind='bar',color='r')\n# ((X_train[y_train==0].position_gain.value_counts().sort_index()/len(y_train))*100).plot(kind='bar',color='g',alpha=0.4)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## Logistic Regression (Logit sm)"],"metadata":{}},{"cell_type":"code","source":["feature_cols = [\n                # 'year',\n                # 'driverId',\n                'points_driver',\n                'median_position',\n                'podiums',\n                'wins', \n                'median_start_position', \n                'num_races',\n                'percentage_races',\n                # 'full_season',\n                # 'constructorId',\n                'position_driver',\n                # 'points_constructor',\n                # 'position_constructor',\n                # 'forename',\n                # 'surname',\n                # 'dob',\n                # 'nationality_driver',\n                'driver_age',\n                # 'constructorRef',\n                # 'name',\n                # 'nationality_constructor',\n                # 'driver_move',\n                # 'percentage_of_constructor', \n                # 'position_gain',\n                # 'nationality_match',\n                # 'podium_scored',\n                # 'out_perform_constructor'\n                ]\nlg_X_train = X_train[feature_cols]\nlg_X_test = X_test[feature_cols]\nlg_X_train = sm.add_constant(lg_X_train)\nlg_X_test = sm.add_constant(lg_X_test)\n\ndriver_model = sm.Logit(y_train, lg_X_train).fit()\ndriver_model.summary()"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["def test_accuracy(cut_off):\n    lg_X_train['binary_pred'] = np.where(lg_X_train['pred'] > cut_off, 1, 0)\n    test_accuracy = accuracy_score(y_train, lg_X_train['binary_pred'])\n    print(f'Test accuracy is {test_accuracy} with cut off of {cut_off}')\n    \ndef accuracy_0to1(list):\n    for i in list:\n        print(test_accuracy(i))"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["lg_X_train['pred'] = driver_model.predict(lg_X_train)\nlg_X_train['binary_pred'] = np.where(lg_X_train['pred'] > 0.50, 1, 0)\nlg_X_test['pred'] = driver_model.predict(lg_X_test)\nlg_X_test['binary_pred'] = np.where(lg_X_test['pred'] > 0.50, 1, 0)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["driver_matrix = confusion_matrix(lg_X_test.binary_pred, y_test)\nprint(type(driver_matrix))\nconf_matrix(driver_matrix)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["print(metrics.classification_report(lg_X_train.binary_pred, y_train))\nprint(metrics.classification_report(lg_X_test.binary_pred, y_test))"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## Logistic Regression (Sklearn)"],"metadata":{}},{"cell_type":"code","source":["feature_cols = [\n                # 'points_driver',\n                'median_position',\n                # 'podiums',\n                # 'wins', \n                # 'median_start_position', \n                # 'num_races',\n                # 'percentage_races',\n                'full_season',\n                # 'position_driver',\n                # 'points_constructor',\n                # 'position_constructor',\n                # 'driver_age',\n                # 'driver_move',\n                'percentage_of_constructor', \n                # 'position_gain',\n                # 'nationality_match',\n                'podium_scored',\n                'out_perform_constructor'\n                ]\n\nlg_X_train = X_train[feature_cols]\nlg_X_test = X_test[feature_cols]\n\nparams = [\n    {\n        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n        'C': np.logspace(-4, 4, 20),\n        'solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n        'max_iter': [100, 500, 1000]\n    }\n]\n\nlg = GridSearchCV(LogisticRegression(), param_grid = params, scoring='accuracy')\nlg.fit(lg_X_train, y_train)\n\ntrain_pred = lg.predict(lg_X_train)\ntest_pred = lg.predict(lg_X_test)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["print(lg.best_params_)\nprint(lg.best_score_)\nprint(lg.best_estimator_)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["print(metrics.classification_report(train_pred, y_train))\ntr_lg_matrix = confusion_matrix(train_pred, y_train)\nconf_matrix(tr_lg_matrix)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["a_p_r(test_pred, y_test)\nte_lg_matrix = confusion_matrix(test_pred, y_test)\nconf_matrix(te_lg_matrix)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## SVM"],"metadata":{}},{"cell_type":"code","source":[""],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## decision trees"],"metadata":{}},{"cell_type":"code","source":["dt_cols = [\n                # 'year',\n                # 'driverId',\n                # 'points_driver',\n                'median_position',\n                # 'podiums',\n                # 'wins', \n                # 'median_start_position', \n                # 'num_races',\n                # 'percentage_races',\n                'full_season',\n                # 'constructorId',\n                'position_driver',\n                # 'points_constructor',\n                # 'position_constructor',\n                # 'forename',\n                # 'surname',\n                # 'dob',\n                # 'nationality_driver',\n                # 'driver_age',\n                # 'constructorRef',\n                # 'name',\n                # 'nationality_constructor',\n                # 'driver_move',\n                'percentage_of_constructor', \n                # 'position_gain',\n                # 'nationality_match',\n                'podium_scored'\n                ]\ndt_X_train = X_train[dt_cols]\ndt_X_test = X_test[dt_cols]\n\ndt = GridSearchCV(estimator = DecisionTreeClassifier(),\n                    param_grid = {'max_depth': [2, 3, 4, 5, 6], # all the parameters we are testing for\n                                  'min_samples_split': [2, 3, 4],\n                                  'min_samples_leaf': [2, 3, 4]},\n                    cv = 5,\n                    refit = True,\n                    verbose = 1, # getting an output\n                    scoring = 'accuracy')\n\ndt.fit(dt_X_train, y_train)\ndt_train_pred = dt.predict(dt_X_train)\ndt_test_pred = dt.predict(dt_X_test)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["print(dt.best_params_)\nprint(dt.best_score_)\nprint(dt.best_estimator_)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["dt = DecisionTreeClassifier(max_depth=3,\n                              min_samples_leaf=4,\n                              min_samples_split=2)\ndt.fit(dt_X_train, y_train)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["fig = plt.figure(figsize=(175,140))\ntree_plot = tree.plot_tree(dt, \n                   feature_names=dt_cols,  \n                   class_names=['Stay', 'Left'],\n                   filled=True)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["print(f'Score on training set: {dt.score(dt_X_train, y_train)}')\nprint(f'Score on testing set: {dt.score(dt_X_test, y_test)}')\nimportance = list(zip(dt_cols, list(dt.feature_importances_)))\nimportance"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["test_results = dt_X_test.copy()\ntest_results['y_pred'] = dt.predict(dt_X_test)\ntest_results['y_real'] = y_test\ntest_results['y_prob'] = dt.predict_proba(dt_X_test)[:,1]\na_p_r(test_results.y_pred, test_results.y_real)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## RandomForest"],"metadata":{}},{"cell_type":"code","source":["rf_cols = [\n                # 'year',\n                # 'driverId',\n                # 'points_driver',\n                'median_position',\n                # 'podiums',\n                # 'wins', \n                # 'median_start_position', \n                # 'num_races',\n                # 'percentage_races',\n                'full_season',\n                # 'constructorId',\n                # 'position_driver',\n                # 'points_constructor',\n                # 'position_constructor',\n                # 'forename',\n                # 'surname',\n                # 'dob',\n                # 'nationality_driver',\n                # 'driver_age',\n                # 'constructorRef',\n                # 'name',\n                # 'nationality_constructor',\n                # 'driver_move',\n                'percentage_of_constructor', \n                'position_gain',\n                # 'nationality_match',\n                'podium_scored',\n                'out_perform_constructor'\n                ]\nrf_X_train = X_train[rf_cols]\nrf_X_test = X_test[rf_cols]\nrandom = 28\nrfc = RandomForestClassifier(random_state=random)\n\nrf_params = {\n    'n_estimators': [7, 8, 9, 10, 11, 12, 13, 14, 15],\n    'max_depth': [2, 3, 4, 5, 6],\n            }\n\n# gs = grid search\ngs = GridSearchCV(rfc, param_grid=rf_params,\n                  scoring = 'accuracy', cv=5)\ngs.fit(rf_X_train, y_train)\nprint(gs.best_score_)\nprint(gs.best_params_)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["driver_rfc = RandomForestClassifier(n_estimators = gs.best_params_.get('n_estimators'),\n                                  max_depth = gs.best_params_.get('max_depth'),\n                                   random_state = random)\ndriver_rfc.fit(rf_X_train, y_train)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["rfc_tr_results = rf_X_train.copy()\nrfc_tr_results['y_pred'] = driver_rfc.predict(rf_X_train)\nrfc_tr_results['y_real'] = y_train\nrfc_tr_results['y_prob'] = driver_rfc.predict_proba(rf_X_train)[:,1]\na_p_r(rfc_tr_results.y_pred, rfc_tr_results.y_real)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["rfc_te_results = rf_X_test.copy()\nrfc_te_results['y_pred'] = driver_rfc.predict(rf_X_test)\nrfc_te_results['y_real'] = y_test\nrfc_te_results['y_prob'] = driver_rfc.predict_proba(rf_X_test)[:,1]\na_p_r(rfc_te_results.y_pred, rfc_te_results.y_real)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["rfc_matrix = confusion_matrix(rfc_te_results.y_pred,\n                                 rfc_te_results.y_real)\nconf_matrix(rfc_matrix)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["rfc_tr_results[rfc_tr_results.y_pred != rfc_tr_results.y_real].sort_values('y_prob')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["col = ['median_position', 'full_season',\n       'percentage_of_constructor',\t'position_gain',\n       'podium_scored', 'out_perform_constructor']\n#modelname.feature_importance_\ny1 = driver_rfc.feature_importances_\n#plot\nfig, ax = plt.subplots() \nwidth = 0.4 # the width of the bars \nind = np.arange(len(y1)) # the x locations for the groups\nax.barh(ind, y1, width, color='#0064dc')\nax.set_yticks(ind+width/10)\nax.set_yticklabels(col, minor=False)\nplt.title('Feature importance in RandomForest Classifier')\nplt.xlabel('Relative importance')\nplt.figure(figsize=(5,5))\nfig.set_size_inches(6.5, 4.5, forward=True)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":[""],"metadata":{}}],"metadata":{"createdWith":"Filament"}}